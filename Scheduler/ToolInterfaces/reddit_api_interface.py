""" Interface for calling the Reddit API from Scheduler

Raises:
    e: _description_
    e: _description_
    e: _description_

Returns:
    _type_: _description_
"""

import logging
import os

from Tools.RedditAPITool.reddit_api_session import RedditAPISession
from Tools.RedditAPITool.reddit_constants import RedditConstants as constants

from atlin_api.atlin_api import Atlin, JobStatus, RedditJobDetails, RedditToken
import Config as config

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def update_quota(job_json, quota_used) -> None:
    """update the quota available to the token

    Args:
        job_json (_type_): dictionary describing the job
        quota_used (_type_): amount of quota used
    """
    try:
        token_uid = job_json['token_uid']

        atlin_session = Atlin(config.ATLIN_API_ADDRESS)
        atlin_session.token_set_quota(token_uid, 'REDDIT', quota_used)

    except Exception as e:
        raise e

    return

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def update_output_path(job_json : dict,
                       output_path : str) -> None:
    """updae the database with the output directory path 

    Args:
        job_json (_type_): dictionary describing the job
        output_path (_type_): directory path to output data

    Raises:
        e: _description_
    """
    logger = logging.getLogger('RedditInterface')

    try:
        atlin_session = Atlin(config.ATLIN_API_ADDRESS)

        job_json['output_path'] = output_path
        job_json['job_status'] = "RUNNING"

        atlin_session.job_update(   job_uid = job_json['job_uid'],
                                    data = job_json)

    except Exception as e:
        logger.error(e)
        raise e

    return

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def update_job_msg(job_json : dict,
                   job_msg : str) -> None:
    """update the job message in the database

    Args:
        job_json (_type_): dictionary describing the job
        job_msg (_type_): job message generated by RedditAPISession

    Raises:
        e: _description_
    """
    logger = logging.getLogger('RedditInterface')

    try:
        atlin_session = Atlin(config.ATLIN_API_ADDRESS)

        job_json['job_message'] = job_msg

        atlin_session.job_update(   job_uid = job_json['job_uid'],
                                    data = job_json)

    except Exception as e:
        logger.error(e)
        raise e

    return

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def generate_output_directory(job_json : dict) -> str:
    """using data from the job_json create a folder to store results
    Args:
        job_json (dict): dictionary describing the job

    Returns:
        str: directory path
    """

    job_uid = job_json['job_uid']

    #Check if the main output directory exists, it if doesn't create it.
    if not os.path.isdir(config.MAIN_OUTPUT_DIR):
        os.mkdir(config.MAIN_OUTPUT_DIR)

    # generate the job specific output directory
    output_path = os.path.join(config.MAIN_OUTPUT_DIR, job_uid)

    #Check if the output directory exists, it if doesn't create it.
    if not os.path.isdir(output_path):
        os.mkdir(output_path)

    return output_path

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def get_credentials_dict_from_db(job_json) -> dict:
    """retrieve reddit credientials from the database

    Args:
        job_json (_type_): dictionary describing the job

    Raises:
        e: _description_

    Returns:
        dict: dictionary in the DB format
    """
    # Make the get request to the API
    atlin_session = Atlin(config.ATLIN_API_ADDRESS)
    atlin_response = None
    try :
        atlin_response = atlin_session.token_get(   user_uid=job_json['user_uid'],
                                                    social_platform=job_json['social_platform'],
                                                    token_uid=job_json['token_uid'])
    except Exception as e:
        logging.error(e)
        raise e

    # extract the token_details from the response if successful
    token_return = {}
    if atlin_response.status_code == 200:
        token_return = atlin_response.json()['token_detail']
    else:
        logging.error('API get request failed with error code: %s', atlin_response.status_code)

    return token_return

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def get_credentials_dict(job_json) -> dict:
    """ This function gets the credientials dict from the database and reforms it into what 
        the tool needs

    Args:
        job_json (_type_): dictionary describing the job

    Returns:
        dict: a dictionary in the format requested by the reddit api
    """
    logging.info('getCredentialsDict: START')

    logging.info('getCredentialsDict: Sending API Get Request for creds')
    token_db = get_credentials_dict_from_db(job_json)
    logging.info('getCredentialsDict: Received')

    token_dict = {   constants.CRED_USERNAME_KEY : token_db['username'],
                    constants.CRED_PASSWORD_KEY : token_db['password'],
                    constants.CRED_CLIENT_ID_KEY : token_db['client_id'],
                    constants.CRED_SECRET_TOKEN_KEY : token_db['secret_token'],
                    constants.CRED_GRANT_TYPE_KEY : constants.CRED_GRANT_TYPE_VALUE}

    logging.info('getCredentialsDict: DONE')

    return token_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def initialize_job_dict() -> dict:
    """initialize the job_dict with default values

    Returns:
        dict: job_dictionary with default values
    """
    job_dict = { constants.REDDIT_JOB_DETAIL_SORT_BY : 'new',
                constants.REDDIT_JOB_DETAIL_TIME_FRAME : 'all',
                constants.REDDIT_JOB_DETAIL_N : 1,
                constants.REDDIT_JOB_DETAIL_SUBREDDIT: 'canada',
                constants.REDDIT_JOB_DETAIL_USER: '',
                constants.REDDIT_JOB_DETAIL_POST: ['', ''],
                constants.REDDIT_JOB_DETAIL_KEYWORD: '',
                constants.REDDIT_JOB_DETAIL_GETPOSTS: 1,
                constants.REDDIT_JOB_DETAIL_COMMENTS: 0}

    return job_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def decode_sort_option(sort_option: str) -> str:
    """convert the dictionary retrieved from the front end into one which
    can be passed to the RedditAPISession

    Args:
        sort_option (str): string received from frontend

    Returns:
        str: string usable by RedditAPISession
    """

    sort_by_str = ''
    time_frame_str = ''

    if sort_option == 'TOP_TODAY':
        sort_by_str = 'top'
        time_frame_str = 'day'
    elif sort_option == 'TOP_WEEK':
        sort_by_str = 'top'
        time_frame_str = 'week'
    elif sort_option == 'TOP_MONTH':
        sort_by_str = 'top'
        time_frame_str = 'month'
    elif sort_option == 'TOP_YEAR':
        sort_by_str = 'top'
        time_frame_str = 'year'
    elif sort_option == 'TOP_ALL':
        sort_by_str = 'top'
        time_frame_str = 'all'
    elif sort_option == 'BEST':
        sort_by_str = 'hot'
        time_frame_str = ''
    elif sort_option == 'KEYWORD':
        sort_by_str = ''
        time_frame_str = ''
    else:
        sort_by_str = 'new'
        time_frame_str = ''

    return sort_by_str, time_frame_str

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def check_keyword(job_dict_db : dict) -> tuple[str, int]:
    """check if keyword job

    Args:
        job_dict_db (dict): job description from front end

    Returns:
        tuple[str, int]: keyword and get value
    """
    keywords = ''
    getposts = 1

    if 'keyword_list' in job_dict_db.keys():
        keywords = job_dict_db['keyword_list']
        getposts = 0

    return keywords, getposts

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def check_get_comments(job_dict_db : dict) -> int:
    """check if the jobs is a comment or post scrape

    Args:
        job_dict_db (dict): job description from front end

    Returns:
        int: do (1) or dont do (0) get comments
    """
    getcomments = 1
    if job_dict_db['scrape_comments'] == 'false':
        getcomments = 0

    return getcomments

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def get_subreddit_job_dict(job_dict_db : dict) -> dict:
    """build a dictionary describing a subreddit scrape request

    Args:
        job_dict_db (dict): job dict from front end

    Returns:
        dict: job dict for RedditAPISession
    """

    sort_by, time_frame = decode_sort_option(job_dict_db['sort_option'])

    keywords, getposts = check_keyword(job_dict_db)

    getcomments = check_get_comments(job_dict_db)

    job_dict = {constants.REDDIT_JOB_DETAIL_SORT_BY : sort_by,
                constants.REDDIT_JOB_DETAIL_TIME_FRAME : time_frame,
                constants.REDDIT_JOB_DETAIL_N : int(job_dict_db['response_count']),
                constants.REDDIT_JOB_DETAIL_SUBREDDIT: job_dict_db['subreddit_list'],
                constants.REDDIT_JOB_DETAIL_USER: '',
                constants.REDDIT_JOB_DETAIL_POST: ['', ''],
                constants.REDDIT_JOB_DETAIL_KEYWORD: keywords,
                constants.REDDIT_JOB_DETAIL_GETPOSTS: getposts,
                constants.REDDIT_JOB_DETAIL_COMMENTS: getcomments}

    return job_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def get_post_job_dict(job_dict_db) -> dict:
    """build a dictionary describing a post scrape request

    Args:
        job_dict_db (_type_):job dict from front end

    Returns:
        _type_: job dict for RedditAPISession
    """

    sort_by, time_frame =decode_sort_option(job_dict_db['sort_option'])
    getcomments = check_get_comments(job_dict_db)

    job_dict = {constants.REDDIT_JOB_DETAIL_SORT_BY : sort_by,
                constants.REDDIT_JOB_DETAIL_TIME_FRAME : time_frame,
                constants.REDDIT_JOB_DETAIL_N : 0,  # this type of request does not need an N value
                constants.REDDIT_JOB_DETAIL_SUBREDDIT: '',
                constants.REDDIT_JOB_DETAIL_USER: '',
                constants.REDDIT_JOB_DETAIL_POST: [job_dict_db['subreddit_name'], job_dict_db['post_list']],
                constants.REDDIT_JOB_DETAIL_KEYWORD: '',
                constants.REDDIT_JOB_DETAIL_GETPOSTS: 1-getcomments,
                constants.REDDIT_JOB_DETAIL_COMMENTS: getcomments}

    return job_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def get_user_job_dict(job_dict_db : dict) -> dict:
    """build a dictionary describing a post scrape request

    Args:
        job_dict_db (_type_):job dict from front end

    Returns:
        _type_: job dict for RedditAPISession
    """
    sort_by, time_frame = decode_sort_option(job_dict_db['sort_option'])
    getcomments = check_get_comments(job_dict_db)

    job_dict = {constants.REDDIT_JOB_DETAIL_SORT_BY : sort_by,
                constants.REDDIT_JOB_DETAIL_TIME_FRAME : time_frame,
                constants.REDDIT_JOB_DETAIL_N : int(job_dict_db['response_count']),
                constants.REDDIT_JOB_DETAIL_SUBREDDIT: '',
                constants.REDDIT_JOB_DETAIL_USER: job_dict_db['username_list'],
                constants.REDDIT_JOB_DETAIL_POST: ['', ''],
                constants.REDDIT_JOB_DETAIL_KEYWORD: '',
                constants.REDDIT_JOB_DETAIL_GETPOSTS: 1-getcomments,
                constants.REDDIT_JOB_DETAIL_COMMENTS: getcomments}

    return job_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def get_job_dict(job_json : dict) -> dict:
    """_summary_

    Args:
        job_json (dict): _description_

    Raises:
        e: _description_

    Returns:
        dict: _description_
    """

    logger = logging.getLogger('RedditInterface')
    job_dict = None

    try:
        job_dict = initialize_job_dict()

        job_dict_db = job_json['job_detail']['job_submit']
        job_type = job_dict_db['option_type']

        if job_type == 'SUBREDDIT':
            job_dict = get_subreddit_job_dict(job_dict_db)
        elif job_type == 'POST':
            job_dict = get_post_job_dict(job_dict_db)
        elif job_type == 'USER':
            job_dict = get_user_job_dict(job_dict_db)
        else:
            logger.info('Unknown option_type: %s',job_type)

    except Exception as e:
        logger.error(e)
        raise e

    return job_dict

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def save_responses(job_json, list_of_responses) -> bool:
    """save the scraped data to a output folder previously generated.

    Args:
        job_json (_type_): _description_
        list_of_responses (_type_): _description_

    Returns:
        bool: _description_
    """
    success_flag = False

    folder_path = job_json['output_path']

    # check if the file path exists and is accessbile then write the listOfResponses_ to the file
    if os.path.exists(folder_path):

        file_path = os.path.join(folder_path, job_json['job_name']+'.json')
        file = open(file_path, "w", encoding='utf-8')

        if len(list_of_responses) == 0:
            file.write('No data found matching scraping criteria.')

        else:
            for resp in list_of_responses:
                file.write(resp.__str__())
                file.write(constants.RESPONSE_BREAK+'\n')

        file.close()

        success_flag = True

    return success_flag

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def reddit_interface(job_json : dict) -> JobStatus:
    """interface for running RedditAPISession based on front end request

    Args:
        job_json (dict): job description fom the front end

    Returns:
        Atlin.JobStatus: status of the job being run 
    """
    # the return value
    a_job_status =  JobStatus().failed

    # initialize logger
    logger = logging.getLogger('RedditInterface')

    job_uid = job_json['job_uid']
    logger.info('Preforming Reddit job: %s', job_uid)
    logger.info('job_json from DB: \n %s', job_json)

    # the credientals  and job details from the JOB_JSON object
    job_dict = get_job_dict(job_json)

    if job_dict is not None:
        logger.info('job_dict: %s', job_dict)

        credentials_dict = get_credentials_dict(job_json)

        # create an output directory to store the collected data in
        output_path = generate_output_directory(job_json)

        update_output_path(job_json, output_path)

        # connect to reddit API
        session = RedditAPISession(credentials_dict)

        # execute the job as defined by the job_dict
        if session.handle_job_dict(job_dict):
            logger.info('Job completed: SUCCESS.')

            if save_responses(job_json, session.list_of_responses):
                logger.info('Job saved.')
                a_job_status = JobStatus().success
            else:
                logging.error('Unable to save job.')
                a_job_status = JobStatus().failed

            # update quota used
            update_quota(job_json, session.number_of_requests)

        else:
            logger.error('Job completed: FAILED.')

        # update the job_message with the message returned form RedditAPISession
        update_job_msg(job_json, session.job_msg)

        # disconnect from the reddit API
        session.End()
        logger.info('session ENDED.')

    return a_job_status
